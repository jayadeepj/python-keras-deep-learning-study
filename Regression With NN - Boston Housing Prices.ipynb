{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression With NN - Boston Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(boston_train_data, boston_train_targets), (boston_test_data, boston_test_targets) = boston_housing.load_data(test_split=0.2,seed=113)\n",
    "\n",
    "# test_split: fraction of the data to reserve as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim/rank/ no of axes of train_data >> 2\n",
      "shape of train_data>> (404, 13)\n",
      "shape of test_data>> (102, 13)\n",
      "\n",
      "\n",
      "dim/rank/ no of axes of train_labels >> 1\n",
      "shape of train_targets>> (404,)\n",
      "shape of test_targets>> (102,)\n",
      "\n",
      "\n",
      "Type of train_data first element >> <class 'numpy.ndarray'>\n",
      "Size of train_data first element >> 13\n",
      "First Record train_data>> [  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]\n",
      "\n",
      "\n",
      "Type of train_targets >> <class 'numpy.ndarray'>\n",
      "Type of train_targets first element >> <class 'numpy.float64'>\n",
      "Size of train_targets>> 404\n",
      "First  of train_targets>> 15.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"dim/rank/ no of axes of train_data >> {boston_train_data.ndim}\")\n",
    "print(f\"shape of train_data>> {boston_train_data.shape}\")\n",
    "print(f\"shape of test_data>> {boston_test_data.shape}\")\n",
    "\n",
    "print('\\n')\n",
    "print(f\"dim/rank/ no of axes of train_labels >> {boston_train_targets.ndim}\")\n",
    "print(f\"shape of train_targets>> {boston_train_targets.shape}\")\n",
    "print(f\"shape of test_targets>> {boston_test_targets.shape}\")\n",
    "\n",
    "print('\\n')\n",
    "print(f\"Type of train_data first element >> {type(boston_train_data[0])}\")\n",
    "print(f\"Size of train_data first element >> {(len(boston_train_data[0]))}\")\n",
    "print(f\"First Record train_data>> {boston_train_data[0,]}\")\n",
    "\n",
    "print('\\n')\n",
    "print(f\"Type of train_targets >> {type(boston_train_targets)}\")\n",
    "print(f\"Type of train_targets first element >> {type(boston_train_targets[0])}\")\n",
    "print(f\"Size of train_targets>> {(len(boston_train_targets))}\")\n",
    "print(f\"First  of train_targets>> {(boston_train_targets)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Exploration - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training Data >> \n",
      "        0     1      2    3      4      5      6       7     8      9     10  \\\n",
      "0  1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0  21.0   \n",
      "1  0.02177  82.5   2.03  0.0  0.415  7.610   15.7  6.2700   2.0  348.0  14.7   \n",
      "2  4.89822   0.0  18.10  0.0  0.631  4.970  100.0  1.3325  24.0  666.0  20.2   \n",
      "3  0.03961   0.0   5.19  0.0  0.515  6.037   34.5  5.9853   5.0  224.0  20.2   \n",
      "4  3.69311   0.0  18.10  0.0  0.713  6.376   88.4  2.5671  24.0  666.0  20.2   \n",
      "\n",
      "       11     12  \n",
      "0  396.90  18.72  \n",
      "1  395.38   3.11  \n",
      "2  375.52   3.26  \n",
      "3  396.90   8.01  \n",
      "4  391.43  14.65  \n",
      " \n",
      "Training Target >> \n",
      "      0\n",
      "0  15.2\n",
      "1  42.3\n",
      "2  50.0\n",
      "3  21.1\n",
      "4  17.7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\" \\nTraining Data >> \")\n",
    "print (pd.DataFrame(boston_train_data).head())\n",
    "print(\" \\nTraining Target >> \")\n",
    "print (pd.DataFrame(boston_train_targets).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation \n",
    "\n",
    "Since the scale of the data varies significantly from one feature to the other, they need to be  normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74511057e+00 1.14801980e+01 1.11044307e+01 6.18811881e-02\n",
      " 5.57355941e-01 6.26708168e+00 6.90106436e+01 3.74027079e+00\n",
      " 9.44059406e+00 4.05898515e+02 1.84759901e+01 3.54783168e+02\n",
      " 1.27408168e+01]\n",
      " \n",
      "Training Data >> \n",
      "        0     1      2    3      4      5      6       7     8      9     10  \\\n",
      "0  1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0  21.0   \n",
      "1  0.02177  82.5   2.03  0.0  0.415  7.610   15.7  6.2700   2.0  348.0  14.7   \n",
      "2  4.89822   0.0  18.10  0.0  0.631  4.970  100.0  1.3325  24.0  666.0  20.2   \n",
      "3  0.03961   0.0   5.19  0.0  0.515  6.037   34.5  5.9853   5.0  224.0  20.2   \n",
      "4  3.69311   0.0  18.10  0.0  0.713  6.376   88.4  2.5671  24.0  666.0  20.2   \n",
      "\n",
      "       11     12  \n",
      "0  396.90  18.72  \n",
      "1  395.38   3.11  \n",
      "2  375.52   3.26  \n",
      "3  396.90   8.01  \n",
      "4  391.43  14.65  \n",
      " \n",
      "Normalised Training Data >> \n",
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.272246 -0.483615 -0.435762 -0.256833 -0.165227 -0.176443  0.813062   \n",
      "1 -0.403427  2.991784 -1.333912 -0.256833 -1.215182  1.894346 -1.910361   \n",
      "2  0.124940 -0.483615  1.028326 -0.256833  0.628642 -1.829688  1.110488   \n",
      "3 -0.401494 -0.483615 -0.869402 -0.256833 -0.361560 -0.324558 -1.236672   \n",
      "4 -0.005634 -0.483615  1.028326 -0.256833  1.328612  0.153642  0.694808   \n",
      "\n",
      "         7         8         9         10        11        12  \n",
      "0  0.116698 -0.626249 -0.595170  1.148500  0.448077  0.825220  \n",
      "1  1.247585 -0.856463 -0.348433 -1.718189  0.431906 -1.329202  \n",
      "2 -1.187439  1.675886  1.565287  0.784476  0.220617 -1.308500  \n",
      "3  1.107180 -0.511142 -1.094663  0.784476  0.448077 -0.652926  \n",
      "4 -0.578572  1.675886  1.565287  0.784476  0.389882  0.263497  \n",
      " \n",
      "Normalised Test Data >> \n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  1.553694 -0.483615  1.028326 -0.256833  1.038381  0.235458  1.110488   \n",
      "1 -0.392427 -0.483615 -0.160878 -0.256833 -0.088401 -0.499474  0.856063   \n",
      "2 -0.399829 -0.483615 -0.869402 -0.256833 -0.361560 -0.397910 -0.846076   \n",
      "3 -0.267805 -0.483615  1.245881  3.893584  0.406700 -0.024096  0.845313   \n",
      "4 -0.398037 -0.483615 -0.972300 -0.256833 -0.924950 -0.206066 -0.437562   \n",
      "\n",
      "         7         8         9         10        11        12  \n",
      "0 -0.939769  1.675886  1.565287  0.784476 -3.484596  2.250921  \n",
      "1 -0.683962 -0.396036  0.157078 -0.307596  0.427331  0.478801  \n",
      "2  0.528643 -0.511142 -1.094663  0.784476  0.448077 -0.414159  \n",
      "3 -0.957671 -0.511142 -0.017443 -1.718189 -0.168767 -0.999345  \n",
      "4  0.003615 -0.741356 -0.956249  0.010925  0.429459 -0.593580  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(boston_train_data.mean(axis=0))\n",
    "\n",
    "boston_mean = boston_train_data.mean(axis=0)\n",
    "boston_std = boston_train_data.std(axis=0)\n",
    "boston_normed_train_data = (boston_train_data - boston_mean)/ boston_std\n",
    "\n",
    "# To normalise the test data, mean/std should be used from the train data & not the test data\n",
    "boston_normed_test_data = (boston_test_data - boston_mean)/ boston_std\n",
    "\n",
    "import pandas as pd\n",
    "print(\" \\nTraining Data >> \")\n",
    "print (pd.DataFrame(boston_train_data).head())\n",
    "\n",
    "print(\" \\nNormalised Training Data >> \")\n",
    "print (pd.DataFrame(boston_normed_train_data).head())\n",
    "\n",
    "print(\" \\nNormalised Test Data >> \")\n",
    "print (pd.DataFrame(boston_normed_test_data).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_regression_network():\n",
    "    boston_model = models.Sequential()\n",
    "    boston_model.add(layers.Dense(64, activation='relu', input_shape=(13,)))\n",
    "    boston_model.add(layers.Dense(64, activation='relu'))\n",
    "    boston_model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    boston_model.compile(optimizer='rmsprop',\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    return boston_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold Cross Validation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data>> (404, 13)\n",
      "No of records in Train>> 202  \n",
      "\"Processing fold: 0, shape of:\n",
      "          train_data >>    (202, 13),\n",
      "          train_targets >> (202,),\n",
      "          val_data >>      (202, 13),\n",
      "          val_targets >>   (202,)\n",
      "History Keys >> dict_keys(['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error'])\n",
      "History Val MAE >> [16.132320411134476, 8.777061169690425, 5.150761212452804, 3.981820741502365, 3.28142247813763, 3.159424508857255, 2.950886081350912, 2.9014931114593354, 2.9258777345761215, 2.813578717779405, 2.917053371667862, 2.7161632746753126, 2.816344657156727, 2.697339021628446, 2.502317920474723, 2.84482609812576, 2.4165265046724, 2.418603831588632, 2.4610011793009123, 2.4182803264938957]\n",
      "\"Processing fold: 1, shape of:\n",
      "          train_data >>    (202, 13),\n",
      "          train_targets >> (202,),\n",
      "          val_data >>      (202, 13),\n",
      "          val_targets >>   (202,)\n",
      "History Keys >> dict_keys(['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error'])\n",
      "History Val MAE >> [17.295376654898767, 9.74176310666717, 5.771652266530707, 4.769306333938448, 4.166469535615184, 3.8193733090221293, 3.558381191574701, 3.380242850520823, 3.312770276671589, 3.1883149268013415, 3.2495403195371724, 3.250679655240314, 2.9841433934646076, 2.985929803092881, 2.923673406095788, 2.8049408794924764, 2.931630720596502, 2.7975328883322157, 2.756221145096392, 2.7079040265024297]\n",
      "\n",
      "The MAE for each fold for each iteration >>> [[16.132320411134476, 8.777061169690425, 5.150761212452804, 3.981820741502365, 3.28142247813763, 3.159424508857255, 2.950886081350912, 2.9014931114593354, 2.9258777345761215, 2.813578717779405, 2.917053371667862, 2.7161632746753126, 2.816344657156727, 2.697339021628446, 2.502317920474723, 2.84482609812576, 2.4165265046724, 2.418603831588632, 2.4610011793009123, 2.4182803264938957], [17.295376654898767, 9.74176310666717, 5.771652266530707, 4.769306333938448, 4.166469535615184, 3.8193733090221293, 3.558381191574701, 3.380242850520823, 3.312770276671589, 3.1883149268013415, 3.2495403195371724, 3.250679655240314, 2.9841433934646076, 2.985929803092881, 2.923673406095788, 2.8049408794924764, 2.931630720596502, 2.7975328883322157, 2.756221145096392, 2.7079040265024297]]\n",
      "\n",
      "The Average MAE >>> 4.1669737260604265\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "k= 2\n",
    "assert(len(boston_normed_train_data)%k == 0)\n",
    "print(f\"shape of train_data>> {boston_normed_train_data.shape}\")\n",
    "boston_fold_len = int(len(boston_normed_train_data)/k)\n",
    "print(f\"No of records in Train>> {boston_fold_len}  \")\n",
    "\n",
    "boston_all_mae_histories = []\n",
    "for i in range(k):\n",
    "    boston_fold_val_data = boston_normed_train_data[i*boston_fold_len:((i+1)*boston_fold_len)]   \n",
    "    boston_fold_val_targets = boston_train_targets[i*boston_fold_len:((i+1)*boston_fold_len)]   \n",
    "    boston_fold_train_data = np.concatenate((boston_normed_train_data[:i*boston_fold_len],\n",
    "                   boston_normed_train_data[((i+1)*boston_fold_len):]),axis=0)\n",
    "    boston_fold_train_targets = np.concatenate((boston_train_targets[:i*boston_fold_len],\n",
    "                   boston_train_targets[((i+1)*boston_fold_len):]),axis=0)\n",
    "    print(f\"\"\"\"Processing fold: {i}, shape of:\n",
    "          train_data >>    {boston_fold_train_data.shape},\n",
    "          train_targets >> {boston_fold_train_targets.shape},\n",
    "          val_data >>      {boston_fold_val_data.shape},\n",
    "          val_targets >>   {boston_fold_val_targets.shape}\"\"\")\n",
    "    \n",
    "    boston_model = build_regression_network()\n",
    "    \n",
    "    boston_history_item = boston_model.fit(x=boston_fold_train_data,\n",
    "                        y=boston_fold_train_targets,\n",
    "                        batch_size=5,\n",
    "                        epochs=20,\n",
    "                        verbose=0,\n",
    "                        validation_data=(boston_fold_val_data,boston_fold_val_targets))\n",
    "    \n",
    "    print(f\"History Keys >> {boston_history_item.history.keys()}\")\n",
    "    print(f\"History Val MAE >> {boston_history_item.history['val_mean_absolute_error']}\")\n",
    "    \n",
    "    boston_mae_history = boston_history_item.history['val_mean_absolute_error']\n",
    "    boston_all_mae_histories.append(boston_mae_history)\n",
    "\n",
    "print(f\"\\nThe MAE for each fold for each iteration >>> {boston_all_mae_histories}\")\n",
    "\n",
    "print(f\"\\nThe Average MAE >>> {np.mean(boston_all_mae_histories)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
